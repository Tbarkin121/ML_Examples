{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole_Actor_Critic NoGradTape",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Hfx5IBeQeurJa5oc9gOLC1ymVh1J8jst",
      "authorship_tag": "ABX9TyPmc0XAgw19EumB1f5QZF1e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tbarkin121/ML_Examples/blob/main/CartPole_Actor_Critic_NoGradTape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwVaXWvCOVtC"
      },
      "source": [
        "https://adventuresinmachinelearning.com/a2c-advantage-actor-critic-tensorflow-2/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDsbtmEROiEW"
      },
      "source": [
        "\"\"\"\n",
        "Classic cart-pole system implemented by Rich Sutton et al.\n",
        "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
        "permalink: https://perma.cc/C9ZM-652R\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import gym\n",
        "from gym import spaces, logger\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CartPoleEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
        "        a frictionless track. The pendulum starts upright, and the goal is to\n",
        "        prevent it from falling over by increasing and reducing the cart's\n",
        "        velocity.\n",
        "    Source:\n",
        "        This environment corresponds to the version of the cart-pole problem\n",
        "        described by Barto, Sutton, and Anderson\n",
        "    Observation:\n",
        "        Type: Box(4)\n",
        "        Num     Observation               Min                     Max\n",
        "        0       Cart Position             -4.8                    4.8\n",
        "        1       Cart Velocity             -Inf                    Inf\n",
        "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
        "        3       Pole Angular Velocity     -Inf                    Inf\n",
        "    Actions:\n",
        "        Type: Discrete(2)\n",
        "        Num   Action\n",
        "        0     Push cart to the left\n",
        "        1     Push cart to the right\n",
        "        Note: The amount the velocity that is reduced or increased is not\n",
        "        fixed; it depends on the angle the pole is pointing. This is because\n",
        "        the center of gravity of the pole increases the amount of energy needed\n",
        "        to move the cart underneath it\n",
        "    Reward:\n",
        "        Reward is 1 for every step taken, including the termination step\n",
        "    Starting State:\n",
        "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
        "    Episode Termination:\n",
        "        Pole Angle is more than 12 degrees.\n",
        "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
        "        the display).\n",
        "        Episode length is greater than 200.\n",
        "        Solved Requirements:\n",
        "        Considered solved when the average return is greater than or equal to\n",
        "        195.0 over 100 consecutive trials.\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\n",
        "        'render.modes': ['human', 'rgb_array'],\n",
        "        'video.frames_per_second': 50\n",
        "    }\n",
        "\n",
        "    def __init__(self):\n",
        "        self.gravity = 9.8\n",
        "        self.masscart = 1.0\n",
        "        self.masspole = 0.1\n",
        "        self.total_mass = (self.masspole + self.masscart)\n",
        "        self.length = 0.5  # actually half the pole's length\n",
        "        self.polemass_length = (self.masspole * self.length)\n",
        "        self.force_mag = 10.0\n",
        "        self.tau = 0.02  # seconds between state updates\n",
        "        self.kinematics_integrator = 'euler'\n",
        "\n",
        "        # Angle at which to fail the episode\n",
        "        self.theta_threshold_radians = 30 * 2 * math.pi / 360\n",
        "        self.x_threshold = 2.4\n",
        "\n",
        "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
        "        # is still within bounds.\n",
        "        high = np.array([self.x_threshold * 2,\n",
        "                         np.finfo(np.float32).max,\n",
        "                         self.theta_threshold_radians * 2,\n",
        "                         np.finfo(np.float32).max],\n",
        "                        dtype=np.float32)\n",
        "\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
        "\n",
        "        self.seed()\n",
        "        self.viewer = None\n",
        "        self.state = None\n",
        "\n",
        "        self.steps_beyond_done = None\n",
        "\n",
        "        self.target_location = 0.0\n",
        "        self.target_weight = 0.0\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, action):\n",
        "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
        "        assert self.action_space.contains(action), err_msg\n",
        "\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        force = self.force_mag if action == 1 else -self.force_mag\n",
        "        costheta = math.cos(theta)\n",
        "        sintheta = math.sin(theta)\n",
        "\n",
        "        # For the interested reader:\n",
        "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
        "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
        "        thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass))\n",
        "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
        "\n",
        "        if self.kinematics_integrator == 'euler':\n",
        "            x = x + self.tau * x_dot\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "        else:  # semi-implicit euler\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            x = x + self.tau * x_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "\n",
        "        self.state = (x, x_dot, theta, theta_dot)\n",
        "\n",
        "        done = bool(\n",
        "            x < -self.x_threshold\n",
        "            or x > self.x_threshold\n",
        "            or theta < -self.theta_threshold_radians\n",
        "            or theta > self.theta_threshold_radians\n",
        "        )\n",
        "        dist_err = self.target_weight*(self.target_location - self.state[0])**2\n",
        "        if not done:\n",
        "            reward = 1.0 - dist_err\n",
        "            \n",
        "        elif self.steps_beyond_done is None:\n",
        "            # Pole just fell!\n",
        "            self.steps_beyond_done = 0\n",
        "            reward = 1.0\n",
        "        else:\n",
        "            if self.steps_beyond_done == 0:\n",
        "                logger.warn(\n",
        "                    \"You are calling 'step()' even though this \"\n",
        "                    \"environment has already returned done = True. You \"\n",
        "                    \"should always call 'reset()' once you receive 'done = \"\n",
        "                    \"True' -- any further steps are undefined behavior.\"\n",
        "                )\n",
        "            self.steps_beyond_done += 1\n",
        "            reward = 0.0\n",
        "\n",
        "        return np.array(self.state), reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
        "        self.steps_beyond_done = None\n",
        "        return np.array(self.state)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        screen_width = 600\n",
        "        screen_height = 400\n",
        "\n",
        "        world_width = self.x_threshold * 2\n",
        "        scale = screen_width/world_width\n",
        "        carty = 100  # TOP OF CART\n",
        "        polewidth = 10.0\n",
        "        polelen = scale * (2 * self.length)\n",
        "        cartwidth = 50.0\n",
        "        cartheight = 30.0\n",
        "\n",
        "        if self.viewer is None:\n",
        "            from gym.envs.classic_control import rendering\n",
        "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
        "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
        "            axleoffset = cartheight / 4.0\n",
        "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
        "            self.carttrans = rendering.Transform()\n",
        "            cart.add_attr(self.carttrans)\n",
        "            self.viewer.add_geom(cart)\n",
        "            l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
        "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
        "            pole.set_color(.8, .6, .4)\n",
        "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
        "            pole.add_attr(self.poletrans)\n",
        "            pole.add_attr(self.carttrans)\n",
        "            self.viewer.add_geom(pole)\n",
        "            self.axle = rendering.make_circle(polewidth/2)\n",
        "            self.axle.add_attr(self.poletrans)\n",
        "            self.axle.add_attr(self.carttrans)\n",
        "            self.axle.set_color(.5, .5, .8)\n",
        "            self.viewer.add_geom(self.axle)\n",
        "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
        "            self.track.set_color(0, 0, 0)\n",
        "            self.viewer.add_geom(self.track)\n",
        "\n",
        "            self._pole_geom = pole\n",
        "\n",
        "        if self.state is None:\n",
        "            return None\n",
        "\n",
        "        # Edit the pole polygon vertex\n",
        "        pole = self._pole_geom\n",
        "        l, r, t, b = -polewidth / 2, polewidth / 2, polelen - polewidth / 2, -polewidth / 2\n",
        "        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
        "\n",
        "        x = self.state\n",
        "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
        "        self.carttrans.set_translation(cartx, carty)\n",
        "        self.poletrans.set_rotation(-x[2])\n",
        "\n",
        "        return self.viewer.render(return_rgb_array=mode == 'rgb_array')\n",
        "\n",
        "    def close(self):\n",
        "        if self.viewer:\n",
        "            self.viewer.close()\n",
        "            self.viewer = None\n",
        "\n",
        "    def set_target(self, target, weight):\n",
        "        self.target_location = target\n",
        "        self.target_weight = weight\n"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KL_tEHvRW7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e782b9ed-305b-48a7-889a-a1b26a93cb58"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install stable-baselines3[extra]"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.7/dist-packages (1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.1.5)\n",
            "Requirement already satisfied: gym>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.19.5)\n",
            "Requirement already satisfied: pillow; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (0.2.9)\n",
            "Requirement already satisfied: opencv-python; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Requirement already satisfied: psutil; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: tensorboard>=2.2.0; extra == \"extra\" in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2018.9)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17->stable-baselines3[extra]) (1.4.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->stable-baselines3[extra]) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.30.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.34.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (56.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.17.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.36.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->stable-baselines3[extra]) (0.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (4.0.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0; extra == \"extra\"->stable-baselines3[extra]) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRtUOq6s8cBN"
      },
      "source": [
        "%%bash\n",
        "# Install additional packages for visualization\n",
        "sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NkSYfWgisKe"
      },
      "source": [
        "import time\n",
        "import datetime, os\n",
        "\n",
        "import collections\n",
        "import gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "\n",
        "# Set seed for experiment reproducibility\n",
        "# seed = 66\n",
        "seed = 8675302\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "# Small epsilon value for stabilizing division operations\n",
        "eps = np.finfo(np.float32).eps.item()"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ELwY3aRjJ6M",
        "outputId": "814eadcc-1291-4c96-8ebe-f138745740f3"
      },
      "source": [
        "# Create the environment\n",
        "# env = gym.make(\"CartPole-v1\")\n",
        "env = CartPoleEnv()\n",
        "env.set_target(target=1.0, weight=0.2)\n",
        "env.seed(seed)\n",
        "\n",
        "# Box(4,) means that it is a Vector with 4 components\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Shape:\", env.observation_space.shape[0])\n",
        "# Discrete(2) means that there is two discrete actions\n",
        "print(\"Action space:\", env.action_space)\n",
        "\n",
        "# Wrap OpenAI Gym's `env.step` call as an operation in a TensorFlow function.\n",
        "# This would allow it to be included in a callable TensorFlow graph.\n",
        "\n",
        "def env_step(action: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"Returns state, reward and done flag given an action.\"\"\"\n",
        "\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  return (state.astype(np.float32), np.array(reward, np.float32), np.array(done, np.int32))\n",
        "\n",
        "\n",
        "def tf_env_step(action: tf.Tensor) -> List[tf.Tensor]:\n",
        "  return tf.numpy_function(env_step, [action], [tf.float32, tf.float32, tf.int32])\n",
        "  \n",
        "\n",
        "num_obs = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "print('num_obs = {}'.format(num_obs))\n",
        "print('num_actions = {}'.format(num_actions))\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Shape: 4\n",
            "Action space: Discrete(2)\n",
            "num_obs = 4\n",
            "num_actions = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnhUgJxIugRk"
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
        "\n",
        "class ExperienceReplay(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acxv2ZvMvYn3"
      },
      "source": [
        "class ActorNet(tf.keras.Model):\n",
        "  \"\"\"Actor network.\"\"\"\n",
        "  def __init__(self, num_actions: int, num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "    # self.actor_input = layers.Input(shape=(4))\n",
        "    self.d1 = layers.Dense(num_hidden_units)\n",
        "    self.lr1 = layers.LeakyReLU()\n",
        "    self.d2 = layers.Dense(num_hidden_units)\n",
        "    self.lr2 = layers.LeakyReLU()\n",
        "    # self.a = layers.Dense(num_actions, activation='softmax')\n",
        "    self.a = layers.Dense(num_actions, activation='tanh')\n",
        "    # self.a = layers.Dense(num_actions, activation='sigmoid')\n",
        "    # self.a = layers.Dense(num_actions)\n",
        "    \n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.d1(inputs)\n",
        "    # x = tf.keras.activations.tanh(x)\n",
        "    x = self.lr1(x)\n",
        "    x = self.d2(x)\n",
        "    x = self.lr2(x)\n",
        "    return self.a(x)\n",
        "\n",
        "\n",
        "class CriticNet(tf.keras.Model):\n",
        "  \"\"\"Critic network.\"\"\"\n",
        "  def __init__(self, num_actions:int, num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "    # self.critic_input = layers.Input(shape=(4))\n",
        "    self.d1 = layers.Dense(num_hidden_units)\n",
        "    self.lr1 = layers.LeakyReLU()\n",
        "    self.d2 = layers.Dense(num_hidden_units)\n",
        "    self.lr2 = layers.LeakyReLU()\n",
        "    # self.critic = layers.Dense(num_actions)\n",
        "    self.c = layers.Dense(1)\n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.d1(inputs)\n",
        "    # x = tf.keras.activations.tanh(x)\n",
        "    x = self.lr1(x)\n",
        "    x = self.d2(x)\n",
        "    x = self.lr2(x)\n",
        "    return self.c(x)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibp9td6ANuCE",
        "outputId": "08d156d7-fd62-4412-ab7d-a7d5b4491a24"
      },
      "source": [
        "from tensorflow.keras.losses import Loss\n",
        "class ActorLoss(Loss):\n",
        "  def call(self, y_true, y_pred):\n",
        "    return tf.reduce_mean((y_pred - y_true)**2, axis=-1)\n",
        "\n",
        "testActorLoss = ActorLoss()\n",
        "y_true = tf.Variable([1,2], dtype='float32')\n",
        "y_pred = tf.Variable([3,3], dtype='float32')\n",
        "print(y_true.shape)\n",
        "loss = testActorLoss(y_true, y_pred)\n",
        "print('loss = {}'.format(loss))\n",
        "\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2,)\n",
            "loss = 2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpTOZ7HFjg-p"
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, gamma = 0.99, mem_size = 100000, batch_size = 1024, mini_batch_size = 128, n_mini_batches=1):\n",
        "        self.gamma = gamma\n",
        "        self.mem_size = mem_size\n",
        "        self.batch_size = batch_size\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.n_mini_batches = n_mini_batches\n",
        "        self.memory = ExperienceReplay(self.mem_size)\n",
        "\n",
        "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        self.c_opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "\n",
        "        self.actor = ActorNet(num_actions, 32)\n",
        "        self.critic = CriticNet(num_actions, 32)\n",
        "\n",
        "        self.actor.compile(\n",
        "                    optimizer='adam')\n",
        "\n",
        "        self.critic.compile(\n",
        "                    optimizer='adam')\n",
        "        \n",
        "        self.huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "        self.I = 1\n",
        "        # Define our metrics\n",
        "        self.actor_loss_metric = tf.keras.metrics.Mean('actor_loss', dtype=tf.float32)\n",
        "        self.critic_loss_metric = tf.keras.metrics.Mean('critic_loss', dtype=tf.float32)\n",
        "        self.logits1_metric = tf.keras.metrics.Mean('logits1', dtype=tf.float32)\n",
        "        self.logits2_metric = tf.keras.metrics.Mean('logits2', dtype=tf.float32)\n",
        "        self.probs1_metric = tf.keras.metrics.Mean('probs1', dtype=tf.float32)\n",
        "        self.probs2_metric = tf.keras.metrics.Mean('probs2', dtype=tf.float32)\n",
        "        \n",
        "        # train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "        # test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')\n",
        "          \n",
        "    def act(self, state, deterministic=False):\n",
        "        logits = self.actor(state)\n",
        "        if (deterministic):\n",
        "            action = tf.argmax(logits, 1)[0]\n",
        "        else:\n",
        "            # action = tf.random.categorical(logits, 1)[0, 0]\n",
        "            # print('logits a = {}'.format(logits))\n",
        "            # logits = logits + 0.1\n",
        "            # print('logits b = {}'.format(logits))\n",
        "            probs = tf.nn.softmax(logits)\n",
        "            log_probs = tf.math.log(probs)\n",
        "            action = tf.random.categorical(log_probs, 1)[0, 0]\n",
        "        return action\n",
        "    def reset_replay_buffer(self):\n",
        "        # Reset memory buffer\n",
        "        self.memory = ExperienceReplay(self.mem_size)\n",
        "    \n",
        "    def fill_replay_buff(self, n_samples):        \n",
        "        # Reset Environment\n",
        "        state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "        # Convert state into a batched tensor (batch size = 1)\n",
        "        state = tf.expand_dims(state, 0)\n",
        "        for i in range(n_samples):\n",
        "          # Run the model and to get action probabilities and critic value\n",
        "          action = self.act(state)\n",
        "\n",
        "          # print('action = {}'.format(action))\n",
        "          # Apply action to the environment to get next state and reward\n",
        "          next_state, reward, done = tf_env_step(action)\n",
        "          next_state = tf.expand_dims(next_state, 0)\n",
        "        \n",
        "          # print('next_state = {}'.format(next_state))\n",
        "          # print('reward = {}'.format(reward))\n",
        "          # print('done = {}'.format(done))\n",
        "          # Store the transition in memory\n",
        "          # print('')\n",
        "          #If Episode is done, reset the environment\n",
        "          if tf.cast(done, tf.bool):\n",
        "            self.memory.push(state, action, next_state, 0, done)\n",
        "            state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "            state = tf.expand_dims(state, 0)\n",
        "          else:\n",
        "            self.memory.push(state, action, next_state, reward, done)\n",
        "            state=next_state\n",
        "\n",
        "    # @tf.function\n",
        "    def train(self, n_mini_batches = 1):\n",
        "        self.fill_replay_buff(self.batch_size)\n",
        "        for _ in range(n_mini_batches):\n",
        "            mini_batch = self.memory.sample(self.mini_batch_size)\n",
        "            # mini_batch = list(agent.memory.memory)[0:self.mini_batch_size]\n",
        "            mini_batch = Transition(*zip(*mini_batch))\n",
        "            state_t1 = tf.reshape(mini_batch[0], shape=(self.mini_batch_size, num_obs))\n",
        "            action_t1 = tf.reshape(mini_batch[1], shape=(self.mini_batch_size, 1))\n",
        "            state_t2 = tf.reshape(mini_batch[2], shape=(self.mini_batch_size, num_obs))\n",
        "            reward_t2 = tf.reshape(mini_batch[3], shape=(self.mini_batch_size, 1))\n",
        "            done = tf.reshape(mini_batch[4], shape=(self.mini_batch_size, 1))\n",
        "            \n",
        "            with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "                logits = self.actor(state_t1, training=True)\n",
        "                values_t1 = self.critic(state_t1, training=True)\n",
        "                values_t2 = self.critic(state_t2, training=True)\n",
        "                # print('logits = {}'.format(logits))\n",
        "                # print('action_t1 = {}'.format(action_t1))\n",
        "                \n",
        "                # print('logit_actions = {}'.format(logit_actions))\n",
        "                # print('logit_actions.shape = {}'.format(logit_actions.shape))\n",
        "                # time.sleep(5)\n",
        "\n",
        "                probs = tf.nn.softmax(logits)\n",
        "                l = logits.numpy()\n",
        "                p = probs.numpy()\n",
        "                self.logits1_metric(l[0])\n",
        "                self.logits2_metric(l[1])\n",
        "                self.probs1_metric(p[0])\n",
        "                self.probs2_metric(p[1])\n",
        "\n",
        "                probs_actions = tf.gather(probs, action_t1, axis=1, batch_dims=1)\n",
        "                log_probs = tf.math.log(probs_actions)\n",
        "                # print('probs_actions = {}'.format(probs_actions))\n",
        "                # print('probs_actions.shape = {}'.format(probs_actions.shape))\n",
        "                # print('log_probs = {}'.format(log_probs))\n",
        "                # print('log_probs.shape = {}'.format(log_probs.shape))\n",
        "                # time.sleep(5)\n",
        "\n",
        "                returns = (tf.cast(reward_t2, 'float32')) + self.gamma*values_t2*(1-tf.cast(done, 'float32'))\n",
        "                # returns = -tf.cast(done, 'float32') + self.gamma*values_t2*(1-tf.cast(done, 'float32'))\n",
        "                advantage =  returns - values_t1 \n",
        "\n",
        "                entropy_loss = -tf.math.reduce_mean(probs_actions*log_probs)\n",
        "                # actor_loss = -log_probs*advantage\n",
        "                # actor_loss = tf.math.reduce_mean(-self.I*log_probs * advantage) - 0.00001*entropy_loss\n",
        "                actor_loss = tf.math.reduce_mean(-log_probs * advantage) - 0.001*entropy_loss\n",
        "                self.I *= self.gamma\n",
        "                critic_loss = 0.5*self.huber_loss(values_t1, returns)\n",
        "                # critic_loss = 0.5*tf.math.reduce_mean(advantage**2)\n",
        "\n",
        "                \n",
        "                \n",
        "            grads1 = tape1.gradient(actor_loss, self.actor.trainable_variables)\n",
        "            grads2 = tape2.gradient(critic_loss, self.critic.trainable_variables)\n",
        "\n",
        "            self.a_opt.apply_gradients(zip(grads1, self.actor.trainable_variables))\n",
        "            self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n",
        "\n",
        "            self.actor_loss_metric(actor_loss)\n",
        "            self.critic_loss_metric(critic_loss)\n",
        "\n",
        "        # return actor_loss, critic_loss\n",
        "    def summary(self):\n",
        "        pass\n",
        "        # state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "        # state = tf.expand_dims(state, 0)\n",
        "        # self.actor.build(input_shape=(1,4))\n",
        "        # self.actor.summary()\n",
        "        # self.critic.build(input_shape=(1,4))\n",
        "        # self.critic.summary()"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9qcxtcw0zJV",
        "outputId": "4365dbf9-d5a1-451d-c310-f181f7c6e720"
      },
      "source": [
        "agent = Agent(mem_size=3, batch_size=100, mini_batch_size=10, n_mini_batches=10)\n",
        "agent.reset_replay_buffer()\n",
        "agent.fill_replay_buff(3)\n",
        "\n",
        "state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "state = tf.expand_dims(state, 0)\n",
        "print(state)\n",
        "# output = agent.actor(state)\n",
        "# output2 = agent.critic(state)\n",
        "# print(output)\n",
        "# print(output2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[-0.0495821  -0.02786041 -0.03143258 -0.03141892]], shape=(1, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKMJ2FBLuwHH"
      },
      "source": [
        "# agent = Agent(mem_size=3, batch_size=100, mini_batch_size=10, n_mini_batches=10)\n",
        "# agent.reset_replay_buffer()\n",
        "# agent.fill_replay_buff(3)\n",
        "# # agent.summary()\n",
        "\n",
        "# for i in range(len(agent.memory)):\n",
        "#   print('mem = {}'.format(i))\n",
        "#   print(agent.memory.memory[i][0])\n",
        "\n",
        "# agent.fill_replay_buff(1)\n",
        "\n",
        "# for i in range(len(agent.memory)):\n",
        "#   print('mem = {}'.format(i))\n",
        "#   print(agent.memory.memory[i][0])\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG0MNGfSS6Xq"
      },
      "source": [
        "# # Trying to log graph... didn't really work\n",
        "# import datetime\n",
        "# stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# logdir = 'logs/func/%s' % stamp\n",
        "# writer = tf.summary.create_file_writer(logdir)\n",
        "\n",
        "# tf.summary.trace_on(graph=True, profiler=True)\n",
        "agent = Agent(mem_size=10000, gamma=0.99, batch_size=501, mini_batch_size=1000, n_mini_batches=10)\n",
        "agent.fill_replay_buff(1000)\n",
        "# agent.actor(state)\n",
        "# with writer.as_default():\n",
        "#   tf.summary.trace_export(\n",
        "#       name=\"my_func_trace\",\n",
        "#       step=0,\n",
        "#       profiler_outdir=logdir)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "WiN-y-vOhMpD",
        "outputId": "d6ddfe15-521e-4978-d224-842a0de1edef"
      },
      "source": [
        "import datetime\n",
        "\n",
        "reward_metric = tf.keras.metrics.Mean('reward', dtype=tf.float32)\n",
        "\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "grand_log_dir = 'logs/gradient_tape/' + current_time + '/grand'\n",
        "# actor_log_dir = 'logs/gradient_tape/' + current_time + '/actor'\n",
        "# critic_log_dir = 'logs/gradient_tape/' + current_time + '/critic'\n",
        "# reward_log_dir = 'logs/gradient_tape/' + current_time + '/reward'\n",
        "grand_summary_writer = tf.summary.create_file_writer(grand_log_dir)\n",
        "# actor_summary_writer = tf.summary.create_file_writer(actor_log_dir)\n",
        "# critic_summary_writer = tf.summary.create_file_writer(critic_log_dir)\n",
        "# reward_summary_writer = tf.summary.create_file_writer(reward_log_dir)\n",
        "\n",
        "\n",
        "min_episodes_criterion = 100\n",
        "max_episodes = 500\n",
        "max_steps_per_episode = 1000\n",
        "\n",
        "# Cartpole-v0 is considered solved if average reward is >= 195 over 100 \n",
        "# consecutive trials\n",
        "reward_threshold = 495\n",
        "running_reward = 0\n",
        "\n",
        "\n",
        "# Keep last episodes reward\n",
        "episodes_reward: collections.deque = collections.deque(maxlen=min_episodes_criterion)\n",
        "env.set_target(-1.9, 0.2)\n",
        "with tqdm.trange(max_episodes) as t:\n",
        "  with grand_summary_writer.as_default():\n",
        "    for i in t:\n",
        "      agent.train(agent.n_mini_batches)\n",
        "      # with actor_summary_writer.as_default():\n",
        "      tf.summary.scalar('actor_loss', agent.actor_loss_metric.result(), step=i)\n",
        "      # with critic_summary_writer.as_default():\n",
        "      tf.summary.scalar('critic_loss', agent.critic_loss_metric.result(), step=i)\n",
        "      tf.summary.scalar('logits1', agent.logits1_metric.result(), step=i)\n",
        "      tf.summary.scalar('logits2', agent.logits2_metric.result(), step=i)\n",
        "      tf.summary.scalar('probs1', agent.probs1_metric.result(), step=i)\n",
        "      tf.summary.scalar('probs2', agent.probs2_metric.result(), step=i)\n",
        "      \n",
        "      agent.actor_loss_metric.reset_states()\n",
        "      agent.critic_loss_metric.reset_states()\n",
        "      agent.logits1_metric.reset_states()\n",
        "      agent.logits2_metric.reset_states()\n",
        "      agent.probs1_metric.reset_states()\n",
        "      agent.probs2_metric.reset_states()\n",
        "      \n",
        "\n",
        "      episode_reward = 0\n",
        "      state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "      state = tf.expand_dims(state, 0)\n",
        "      logits = agent.actor(state)\n",
        "      probs = tf.nn.softmax(logits)\n",
        "      log_probs = tf.math.log(probs)\n",
        "      p = probs.numpy()\n",
        "      lp = log_probs.numpy()\n",
        "      # print('p(a1) = {}. p(a2) = {}'.format(lp[0][0],lp[0][1]))\n",
        "      for _ in range(500):\n",
        "          action = agent.act(state, deterministic=True)\n",
        "\n",
        "          # print('action_logits_t = {}'.format(action_logits_t))\n",
        "          \n",
        "          # print('action = {}'.format(action))\n",
        "          # Apply action to the environment to get next state and reward\n",
        "          state, reward, done = tf_env_step(action)\n",
        "          state = tf.expand_dims(state, 0)\n",
        "          episode_reward += reward\n",
        "          if (tf.cast(done, tf.bool)):\n",
        "              env.reset()\n",
        "              break\n",
        "      reward_metric(episode_reward)\n",
        "      # with reward_summary_writer.as_default():\n",
        "      tf.summary.scalar('episode reward', reward_metric.result(), step=i)\n",
        "      reward_metric.reset_states()\n",
        "\n",
        "      episodes_reward.append(episode_reward.numpy())\n",
        "      running_reward = statistics.mean(episodes_reward)\n",
        "\n",
        "      t.set_description(f'Episode {i}')\n",
        "      t.set_postfix(episode_reward=episode_reward.numpy(), running_reward=running_reward, pa0 = p[0][0], pa1 = p[0][1])\n",
        "\n",
        "      # Show average episode reward every 10 episodes\n",
        "      if i % 10 == 0:\n",
        "        pass\n",
        "        # print(f'Episode {i}: average reward: {avg_reward}')\n",
        "\n",
        "      if running_reward > reward_threshold and i >= min_episodes_criterion:  \n",
        "        break\n",
        "\n",
        "print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 65:  13%|█▎        | 66/500 [01:32<10:07,  1.40s/it, episode_reward=102, pa0=0.298, pa1=0.702, running_reward=45.7]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-833023fdc15f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mgrand_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m       \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mini_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m       \u001b[0;31m# with actor_summary_writer.as_default():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'actor_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_loss_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-3a2086a911c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_mini_batches)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mgrads1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mgrads2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_GatherV2Grad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     params_grad = _BatchGatherGrad(params_shape, values_transpose, indices,\n\u001b[0;32m--> 697\u001b[0;31m                                    batch_dims, params_shape[axis])\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Inverts the above transpose by moving dimension batch_dims back to its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_BatchGatherGrad\u001b[0;34m(params_shape, values, indices, batch_dims, gather_dim_size)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0minner_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_dims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m     \u001b[0mflat_values_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_shape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m     \u001b[0mgather_dim_size\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1766\u001b[0m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[1;32m   1767\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1216\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m       return concat_v2_eager_fallback(\n\u001b[0;32m-> 1218\u001b[0;31m           values, axis, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   1219\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2_eager_fallback\u001b[0;34m(values, axis, name, ctx)\u001b[0m\n\u001b[1;32m   1247\u001b[0m   \u001b[0m_attr_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m   \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m   \u001b[0m_attr_Tidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs_to_matching_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"N\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_N\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tidx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_attr_Tidx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36margs_to_matching_eager\u001b[0;34m(l, ctx, allowed_dtypes, default_dtype)\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0;31m# not list allowed dtypes, in which case we should skip this.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mallowed_dtypes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;31m# If we did not match an allowed dtype, try again with the default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# dtype. This could be because we have an empty tensor and thus we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhjmDBanDKf_"
      },
      "source": [
        "action = agent.act(state)\n",
        "actor_out = agent.actor(state)\n",
        "print(actor_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHpKh5Jk_X9v"
      },
      "source": [
        "print(agent.actor(state))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNZxG7SfkyOd"
      },
      "source": [
        "# Render an episode and save as a GIF file\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "from pyvirtualdisplay import Display\n",
        "from datetime import datetime\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "\n",
        "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
        "  screen = env.render(mode='rgb_array')\n",
        "  im = Image.fromarray(screen)\n",
        "\n",
        "  images = [im]\n",
        "\n",
        "  state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "  state = tf.expand_dims(state, 0)\n",
        "  for i in range(1, 500 + 1):\n",
        "\n",
        "    action = agent.act(state, deterministic=True)\n",
        "    # action = agent.act(state, deterministic=False)\n",
        "    state, reward, done = tf_env_step(action)\n",
        "    state = tf.expand_dims(state, 0)\n",
        "    print('reward = {}'.format(reward))\n",
        "    # Render screen every n steps\n",
        "    n=2\n",
        "    if i % n == 0:\n",
        "      screen = env.render(mode='rgb_array')\n",
        "      images.append(Image.fromarray(screen))\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  return images\n",
        "\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(env, agent.actor, max_steps_per_episode)\n",
        "image_file = 'cartpole-v1.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)\n",
        "\n",
        "\n",
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3EMXF_rGoRU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xrf16iIiLc6"
      },
      "source": [
        "!zip -r /content/file.zip /content/logs/\n",
        "# from google.colab import files\n",
        "# files.download(\"/content/file.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InfxMYhSuXnW"
      },
      "source": [
        "# rm -r logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-GhRx5ZpxgI"
      },
      "source": [
        "# %load_ext tensorboard\n",
        "# %reload_ext tensorboard\n",
        "%tensorboard --logdir ./logs/func"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTYxohRIJKvT"
      },
      "source": [
        "class ActorNet(tf.keras.Model):\n",
        "  \"\"\"Actor network.\"\"\"\n",
        "  def __init__(self, num_actions: int, num_hidden_units: int):\n",
        "    \"\"\"Initialize.\"\"\"\n",
        "    super().__init__()\n",
        "    # self.actor_input = layers.Input(shape=(4))\n",
        "    self.d1 = layers.Dense(num_hidden_units)\n",
        "    self.lr1 = layers.LeakyReLU()\n",
        "    self.d2 = layers.Dense(num_hidden_units)\n",
        "    self.lr2 = layers.LeakyReLU()\n",
        "    # self.a = layers.Dense(num_actions, activation='softmax')\n",
        "    self.a = layers.Dense(num_actions, activation='tanh')\n",
        "    # self.a = layers.Dense(num_actions, activation='sigmoid')\n",
        "    # self.a = layers.Dense(num_actions)\n",
        "    \n",
        "\n",
        "  def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "    x = self.d1(inputs)\n",
        "    # x = tf.keras.activations.tanh(x)\n",
        "    x = self.lr1(x)\n",
        "    x = self.d2(x)\n",
        "    x = self.lr2(x)\n",
        "    return self.a(x)\n",
        "\n",
        "\n",
        "# inputs = tf.keras.Input(shape=(3,))\n",
        "# x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)\n",
        "# outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)\n",
        "# model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model = ActorNet(1,2)\n",
        "model.build(input_shape=[1, 3])\n",
        "x = tf.random.uniform((1, 3))\n",
        "test = model(x)\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-352lhR9B3u"
      },
      "source": [
        "# The function to be traced.\n",
        "@tf.function\n",
        "def eval_model(x):\n",
        "  # A simple hand-rolled layer.\n",
        "  return model(x)\n",
        "\n",
        "# Set up logging.\n",
        "stamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = 'logs/func/%s' % stamp\n",
        "writer = tf.summary.create_file_writer(logdir)\n",
        "\n",
        "# Sample data for your function.\n",
        "x = tf.random.uniform((1, 3))\n",
        "\n",
        "# Bracket the function call with\n",
        "# tf.summary.trace_on() and tf.summary.trace_export().\n",
        "# tf.summary.trace_on(graph=True, profiler=True)\n",
        "tf.profiler.experimental.stop(logdir)\n",
        "tf.profiler.experimental.start(logdir)\n",
        "tf.profiler.experimental.trace(\"Train\", step_num=1):\n",
        "  z = eval_model(x)\n",
        "# with writer.as_default():\n",
        "#   tf.summary.trace_export(\n",
        "#       name=\"my_func_trace\",\n",
        "#       step=0,\n",
        "#       profiler_outdir=logdir)\n",
        "tf.profiler.experimental.stop(logdir)\n",
        "\n",
        "# Call only one tf.function when tracing.\n",
        "# z = eval_model(x)\n",
        "# with writer.as_default():\n",
        "#   tf.summary.trace_export(\n",
        "#       name=\"my_func_trace\",\n",
        "#       step=0,\n",
        "#       profiler_outdir=logdir)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}